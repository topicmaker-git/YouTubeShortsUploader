"""
YouTube Shorts ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
è¤‡æ•°ã®å‹•ç”»ã‚’ä¸€æ‹¬ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import os
import time
import csv
import glob
import shutil
import logging
from datetime import datetime, timedelta
from .uploader import upload_with_retry
from .playlist_manager import PlaylistManager


class ShortsBatchUploader:
    """YouTube Shorts ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¹"""

    def __init__(self, youtube_client):
        """
        åˆæœŸåŒ–

        Args:
            youtube_client: YouTube API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        """
        self.youtube = youtube_client
        self.upload_history = []
        self.playlist_manager = PlaylistManager(youtube_client)

    def schedule_upload(self, video_files, interval_minutes=30, metadata_list=None):
        """
        æŒ‡å®šã•ã‚ŒãŸé–“éš”ã§å‹•ç”»ã‚’äºˆç´„æŠ•ç¨¿ã™ã‚‹

        Args:
            video_files (list): å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
            interval_minutes (int): æŠ•ç¨¿é–“éš”ï¼ˆåˆ†ï¼‰
            metadata_list (list): ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰

        Returns:
            list: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœã®ãƒªã‚¹ãƒˆ
        """
        scheduled_time = datetime.now()
        results = []

        print(f"\n=== ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹ ===")
        print(f"å¯¾è±¡å‹•ç”»æ•°: {len(video_files)}")
        print(f"æŠ•ç¨¿é–“éš”: {interval_minutes}åˆ†\n")

        for i, video_file in enumerate(video_files):
            print(f"\n[{i + 1}/{len(video_files)}] {os.path.basename(video_file)}")

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã¾ãŸã¯è‡ªå‹•ç”Ÿæˆ
            if metadata_list and i < len(metadata_list):
                metadata = metadata_list[i]
            else:
                metadata = self.generate_metadata(video_file)

            # äºˆç´„æŠ•ç¨¿æ™‚åˆ»ã‚’è¨­å®š
            metadata['scheduled_time'] = scheduled_time

            # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
            result = upload_with_retry(self.youtube, video_file, metadata)

            if result:
                # å±¥æ­´ã«è¿½åŠ 
                history_entry = {
                    'file': video_file,
                    'video_id': result['id'],
                    'title': result['title'],
                    'url': result['url'],
                    'uploaded_at': datetime.now().isoformat(),
                    'scheduled_for': scheduled_time.isoformat(),
                    'privacy_status': result['privacy_status']
                }
                self.upload_history.append(history_entry)
                results.append(result)

                print(f"âœ“ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {result['url']}")
            else:
                print(f"âœ— ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {video_file}")

            # æ¬¡ã®æŠ•ç¨¿æ™‚åˆ»ã‚’è¨ˆç®—
            scheduled_time += timedelta(minutes=interval_minutes)

            # APIåˆ¶é™ã‚’è€ƒæ…®ã—ã¦å¾…æ©Ÿï¼ˆæœ€å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ä»¥å¤–ï¼‰
            if i < len(video_files) - 1:
                wait_time = 10
                print(f"\n{wait_time}ç§’å¾…æ©Ÿã—ã¦ã‹ã‚‰æ¬¡ã®å‹•ç”»ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™...")
                time.sleep(wait_time)

        print(f"\n=== ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº† ===")
        print(f"æˆåŠŸ: {len(results)}/{len(video_files)}")

        return results

    def upload_from_directory(self, directory, pattern='*.mp4', **kwargs):
        """
        æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰

        Args:
            directory (str): å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            pattern (str): ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: '*.mp4'ï¼‰
            **kwargs: schedule_uploadã«æ¸¡ã™è¿½åŠ å¼•æ•°

        Returns:
            list: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœã®ãƒªã‚¹ãƒˆ
        """
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        search_pattern = os.path.join(directory, pattern)
        video_files = sorted(glob.glob(search_pattern))

        if not video_files:
            print(f"è­¦å‘Š: {search_pattern} ã«ä¸€è‡´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return []

        print(f"{len(video_files)}å€‹ã®å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        for i, video_file in enumerate(video_files, 1):
            print(f"  {i}. {os.path.basename(video_file)}")

        return self.schedule_upload(video_files, **kwargs)

    def upload_from_csv(self, csv_file):
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰

        CSVå½¢å¼:
        file,title,description,tags,category_id,privacy_status,playlist_name,publish_at

        Args:
            csv_file (str): CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

        Returns:
            list: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœã®ãƒªã‚¹ãƒˆ
        """
        video_files = []
        metadata_list = []

        print(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™: {csv_file}")

        try:
            with open(csv_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    video_files.append(row['file'])

                    # ã‚¿ã‚°ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
                    tags = []
                    if 'tags' in row and row['tags']:
                        tags = [tag.strip() for tag in row['tags'].split(',')]

                    # å†ç”Ÿãƒªã‚¹ãƒˆåã‹ã‚‰IDã‚’å–å¾—ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯è­¦å‘Šã®ã¿ï¼‰
                    playlist_id = None
                    if 'playlist_name' in row and row['playlist_name']:
                        playlist_id = self.playlist_manager.get_playlist(row['playlist_name'])
                        if not playlist_id:
                            print(f"è­¦å‘Š: å†ç”Ÿãƒªã‚¹ãƒˆ '{row['playlist_name']}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                            print(f"      å‹•ç”»ã¯å†ç”Ÿãƒªã‚¹ãƒˆãªã—ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ã€‚")
                            print(f"      YouTube Studioã§å¾Œã‹ã‚‰è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")

                    metadata = {
                        'title': row.get('title', ''),
                        'description': row.get('description', ''),
                        'tags': tags,
                        'category_id': row.get('category_id', '22'),
                        'privacy_status': row.get('privacy_status', 'public'),
                        'playlist_id': playlist_id,
                        'publish_at': row.get('publish_at', None)  # ISO 8601å½¢å¼: "2025-11-20T10:00:00Z"
                    }
                    metadata_list.append(metadata)

            print(f"{len(video_files)}ä»¶ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            return self.schedule_upload(video_files, metadata_list=metadata_list)

        except FileNotFoundError:
            print(f"ã‚¨ãƒ©ãƒ¼: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_file}")
            return []
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return []

    def generate_metadata(self, video_file):
        """
        å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•ç”Ÿæˆ

        Args:
            video_file (str): å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

        Returns:
            dict: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        """
        filename = os.path.basename(video_file)
        base_name = os.path.splitext(filename)[0]

        return {
            'title': f"{base_name}",
            'description': f"è‡ªå‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸShortså‹•ç”»ã§ã™ã€‚\næŠ•ç¨¿æ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}",
            'tags': ['Shorts', 'è‡ªå‹•æŠ•ç¨¿'],
            'category_id': '22',
            'privacy_status': 'public'
        }

    def save_history(self, filename='upload_history.csv'):
        """
        ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’CSVã«ä¿å­˜

        Args:
            filename (str): ä¿å­˜å…ˆãƒ•ã‚¡ã‚¤ãƒ«å
        """
        if not self.upload_history:
            print("ä¿å­˜ã™ã‚‹å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        try:
            # logsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
            if not filename.startswith('logs/'):
                filename = os.path.join('logs', filename)

            with open(filename, 'w', newline='', encoding='utf-8') as f:
                fieldnames = self.upload_history[0].keys()
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(self.upload_history)

            print(f"\nã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: å±¥æ­´ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def get_statistics(self):
        """
        ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµ±è¨ˆã‚’å–å¾—

        Returns:
            dict: çµ±è¨ˆæƒ…å ±
        """
        if not self.upload_history:
            return None

        total = len(self.upload_history)
        privacy_counts = {}

        for entry in self.upload_history:
            privacy = entry['privacy_status']
            privacy_counts[privacy] = privacy_counts.get(privacy, 0) + 1

        return {
            'total_uploads': total,
            'privacy_status_breakdown': privacy_counts,
            'first_upload': self.upload_history[0]['uploaded_at'],
            'last_upload': self.upload_history[-1]['uploaded_at']
        }

    def upload_from_csv_scheduled(self, csv_file, max_uploads=5, log_file=None):
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å…ˆé ­Nä»¶ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€å‡¦ç†æ¸ˆã¿è¡Œã‚’å‰Šé™¤
        Windowsã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã§ã®å®šæœŸå®Ÿè¡Œã«æœ€é©

        Args:
            csv_file (str): CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            max_uploads (int): 1å›ã®å®Ÿè¡Œã§å‡¦ç†ã™ã‚‹æœ€å¤§ä»¶æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰
            log_file (str): ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰

        Returns:
            dict: å®Ÿè¡Œçµæœã®ã‚µãƒãƒªãƒ¼
        """
        # ãƒ­ã‚°è¨­å®š
        if log_file is None:
            log_dir = 'logs'
            os.makedirs(log_dir, exist_ok=True)
            log_file = os.path.join(log_dir, f'scheduled_upload_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )

        logger = logging.getLogger(__name__)
        logger.info("=" * 60)
        logger.info("ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹")
        logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«: {csv_file}")
        logger.info(f"æœ€å¤§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ•°: {max_uploads}")
        logger.info("=" * 60)

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        if not os.path.exists(csv_file):
            logger.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_file}")
            return {'success': False, 'error': 'CSV file not found'}

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        try:
            with open(csv_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                all_rows = list(reader)
                fieldnames = reader.fieldnames
        except Exception as e:
            logger.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return {'success': False, 'error': str(e)}

        if not all_rows:
            logger.info("CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã™ã¹ã¦ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¦ã„ã¾ã™ã€‚")
            return {'success': True, 'uploaded': 0, 'remaining': 0, 'message': 'All uploads completed'}

        # å‡¦ç†ã™ã‚‹è¡Œæ•°ã‚’æ±ºå®š
        rows_to_process = all_rows[:max_uploads]
        remaining_rows = all_rows[max_uploads:]

        logger.info(f"CSVå†…ã®ç·è¡Œæ•°: {len(all_rows)}")
        logger.info(f"ä»Šå›å‡¦ç†ã™ã‚‹è¡Œæ•°: {len(rows_to_process)}")
        logger.info(f"å‡¦ç†å¾Œã®æ®‹ã‚Šè¡Œæ•°: {len(remaining_rows)}")
        logger.info("")

        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœ
        success_count = 0
        failed_count = 0
        results = []

        # å„è¡Œã‚’å‡¦ç†
        for i, row in enumerate(rows_to_process, 1):
            logger.info(f"[{i}/{len(rows_to_process)}] å‡¦ç†ä¸­: {row.get('file', 'N/A')}")

            try:
                video_file = row['file']

                # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
                if not os.path.exists(video_file):
                    logger.warning(f"å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {video_file}")
                    failed_count += 1
                    results.append({
                        'file': video_file,
                        'status': 'failed',
                        'error': 'File not found'
                    })
                    continue

                # ã‚¿ã‚°ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
                tags = []
                if 'tags' in row and row['tags']:
                    tags = [tag.strip() for tag in row['tags'].split(',')]

                # å†ç”Ÿãƒªã‚¹ãƒˆå‡¦ç†
                playlist_id = None
                if 'playlist_name' in row and row['playlist_name']:
                    playlist_id = self.playlist_manager.get_playlist(row['playlist_name'])
                    if not playlist_id:
                        logger.warning(f"å†ç”Ÿãƒªã‚¹ãƒˆ '{row['playlist_name']}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                        logger.warning("å‹•ç”»ã¯å†ç”Ÿãƒªã‚¹ãƒˆãªã—ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ã€‚")

                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
                metadata = {
                    'title': row.get('title', ''),
                    'description': row.get('description', ''),
                    'tags': tags,
                    'category_id': row.get('category_id', '22'),
                    'privacy_status': row.get('privacy_status', 'public'),
                    'playlist_id': playlist_id,
                    'publish_at': row.get('publish_at', None)
                }

                # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
                result = upload_with_retry(self.youtube, video_file, metadata)

                if result:
                    logger.info(f"âœ“ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {result['id']}")
                    logger.info(f"  URL: {result['url']}")
                    success_count += 1
                    results.append({
                        'file': video_file,
                        'status': 'success',
                        'video_id': result['id'],
                        'url': result['url']
                    })
                else:
                    logger.error(f"âœ— ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {video_file}")
                    failed_count += 1
                    results.append({
                        'file': video_file,
                        'status': 'failed',
                        'error': 'Upload failed'
                    })

            except Exception as e:
                logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                failed_count += 1
                results.append({
                    'file': row.get('file', 'N/A'),
                    'status': 'failed',
                    'error': str(e)
                })

            # APIåˆ¶é™ã‚’è€ƒæ…®ã—ã¦å¾…æ©Ÿ
            if i < len(rows_to_process):
                wait_time = 10
                logger.info(f"{wait_time}ç§’å¾…æ©Ÿ...")
                time.sleep(wait_time)

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ï¼ˆå‡¦ç†æ¸ˆã¿è¡Œã‚’å‰Šé™¤ï¼‰
        try:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ
            backup_file = csv_file + '.backup'
            shutil.copy2(csv_file, backup_file)
            logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆã—ã¾ã—ãŸ: {backup_file}")

            # æ®‹ã‚Šã®è¡Œã§CSVã‚’ä¸Šæ›¸ã
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(remaining_rows)

            logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã—ã¾ã—ãŸ: å‡¦ç†æ¸ˆã¿{len(rows_to_process)}è¡Œã‚’å‰Šé™¤")

        except Exception as e:
            logger.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            logger.error("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å¾©å…ƒã—ã¦ãã ã•ã„")

        # ã‚µãƒãƒªãƒ¼
        logger.info("")
        logger.info("=" * 60)
        logger.info("å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼")
        logger.info("=" * 60)
        logger.info(f"æˆåŠŸ: {success_count}")
        logger.info(f"å¤±æ•—: {failed_count}")
        logger.info(f"æ®‹ã‚Šã®å‹•ç”»æ•°: {len(remaining_rows)}")
        logger.info(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
        logger.info("=" * 60)

        if len(remaining_rows) == 0:
            logger.info("ğŸ‰ ã™ã¹ã¦ã®å‹•ç”»ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

        return {
            'success': True,
            'uploaded': success_count,
            'failed': failed_count,
            'remaining': len(remaining_rows),
            'log_file': log_file,
            'results': results
        }


if __name__ == '__main__':
    # ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰
    import sys

    # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

    from src.auth import authenticate_youtube

    print("=== YouTube Shorts ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ ===\n")

    # èªè¨¼
    youtube = authenticate_youtube()

    # ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ã®åˆæœŸåŒ–
    uploader = ShortsBatchUploader(youtube)

    print("\nä½¿ç”¨ä¾‹:")
    print("1. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰:")
    print("   uploader.upload_from_directory('shorts_videos', interval_minutes=30)")
    print("\n2. CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰:")
    print("   uploader.upload_from_csv('upload_list.csv')")
    print("\n3. ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’ä¿å­˜:")
    print("   uploader.save_history('upload_history.csv')")
